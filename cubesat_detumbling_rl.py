# Librer铆as a utilizar
import gymnasium as gym
from gymnasium import spaces
import numpy as np
import time


import matplotlib
matplotlib.use("TkAgg")  # Backend interactivo
import matplotlib.pyplot as plt
import matplotlib
from zmq.backend import second

plt.close('all')

matplotlib.use('TkAgg')
import matplotlib.pyplot as plt
from datetime import datetime, timedelta
from dotenv import load_dotenv

load_dotenv()

# Importar componentes existentes del simulador de HoneySat
from Simulations.RotationSimulation import RotationSimulation
from Simulations.OrbitalSimulation import OrbitalSimulation
from Simulations.MagneticSimulation import MagneticSimulation
from SatellitePersonality import SatellitePersonality


class CubeSatDetumblingEnv(gym.Env):
    """
    ENTORNO DE GYMNASIUM PARA PROBLEMA DE DETUMBLING USANDO SIMULADOR DE HONEYSAT.

    Este entorno se integra con las clases existentes de RotationSimulation, OrbitalSimulation
    y MagneticSimulation para proporcionar una simulaci贸n realista de la din谩mica de sat茅lites
    para el aprendizaje por refuerzo.
    """

    metadata = {'render_modes': ['human', 'none']}

    def __init__(self, render_mode=None, max_steps=400, start_time=datetime.now(), time_step=0.1, granularity=40, debug=False, num_bins=4, plot_hist=False):
        """
        Inicializar el entorno de CubeSat para el problema de detumbling.

        Args:
            render_mode (str): Modo de renderizado ('human' o None)
            max_steps (int): Pasos m谩ximos por episodio
            start_time (datetime): Tiempo inicial de la simulacion
            time_step (float): Paso de tiempo de simulaci贸n en segundos
            granularity (int): Granularida de la simulacion, divide a time_step
            debug (bool): Activar historico de observaciones y graficar
        """
        super().__init__()

        self.render_mode = render_mode
        self.max_steps = max_steps
        self.time_step = time_step
        self.current_time = start_time
        self.sim_granularity = granularity
        self._plot_hist = plot_hist

        # inicializar componentes del simulador
        self.rotation_sim = None
        self.orbital_sim = None
        self.magnetic_sim = None

        self.num_bins = num_bins

        # Discretize the action space for Q-learning
        # Actions: Positive/Negative torque on each axis (X, Y, Z) + No torque
        self.max_torque = SatellitePersonality.MAX_TORQUE_REACTION_WHEEL
        self.action_map = self.create_action_map_xyz()
        self.action_space = spaces.Discrete(len(self.action_map))

        # definir espacio de observaciones
        # box: quaternion (4) + velocidad angular (3) + campo magnetico (3)
        self.observation_space = spaces.Box(
            low=-np.inf,
            high=np.inf,
            shape=(10,),
            dtype=np.float32
        )

        # tracking dentro de un episodio
        self.current_step = 0
        self.episode_reward = 0.0

        # como es casi imposible tener velocidad angular cero, se establece un umbral
        # ajustable dependiendo de la misi贸n y el contexto
        self.success_threshold = 0.01  # rad/s

        # Para efectos de debug, guardar un historial de observaciones y graficarlas
        self._debug = debug  # Debug activado
        self._observation_hist = []  # Historico de observaciones
        self._time_hist = []  # Historic time
        if self._debug:
            # import matplotlib.pyplot as plt
            pass
            # self.__figure, axes = plt.subplots(2, 1)
            # axes[0].grid(True)
            # axes[1].grid(True)
            # plt.ion()
            # plt.show(block=False)
    
    def create_action_map_xyz(self):
        """
        Crear el action_map para los tres ejes X, Y y Z usando discretizaci贸n logar铆tmica.
        M谩s bins cerca de cero para control fino.
        """
        action_map = {}

        min_torque = self.max_torque
        max_torque = self.max_torque

        # Crear bins logar铆tmicos para un solo eje
        positive_bins = np.linspace(min_torque, max_torque, self.num_bins // 4)
        negative_bins = -positive_bins[::-1]  # los negativos

        positive_bins_z = np.linspace(max_torque / 8, max_torque, self.num_bins // 2)
        negative_bins_z = -positive_bins_z[::-1]  # los negativos

        # Incluimos el cero
        axis_bins = np.concatenate([negative_bins, positive_bins])
        axis_bins_z = np.concatenate([negative_bins_z, positive_bins_z])

        # print(axis_bins)

        # Crear todas las combinaciones posibles de X, Y y Z
        index = 0

        for tx in axis_bins:
            action_map[index] = np.array([tx, 0, 0])
            print(action_map[index])
            index += 1
        
        for ty in axis_bins:
            action_map[index] = np.array([0, ty, 0])
            print(action_map[index])
            index += 1

        for tz in axis_bins_z:
            action_map[index] = np.array([0, 0, tz])
            print(action_map[index])
            index += 1

        # print(action_map)
        return action_map


    def _create_simulators(self):
        """Crear instancias de simuladores.
        - RotationSimulation
        - OrbitalSimulation
        - MagneticSimulation
        """
        if self.rotation_sim is not None:
            try:
                self.rotation_sim.stop()
            except Exception:
                pass
        if self.orbital_sim is not None:
            try:
                self.orbital_sim.stop()
            except Exception:
                pass
        if self.magnetic_sim is not None:
            try:
                self.magnetic_sim.stop()
            except Exception:
                pass

        # llamar constructores de simuladores, ver parametros si se necesita debuguear
        self.rotation_sim = RotationSimulation(debug=False)
        self.orbital_sim = OrbitalSimulation(self.rotation_sim)
        self.magnetic_sim = MagneticSimulation(self.orbital_sim, self.rotation_sim)

    def _start_simulators(self):
        """Inicializar hilos de cada simulador. Implementaci贸n paralelizada."""
        #TODO: Do not start the simulation thread, control them manually instead
        try:
            self.rotation_sim.start()
            self.orbital_sim.start()
            self.magnetic_sim.start()
        except Exception as e:
            print(f"Warning: Could not start all simulators: {e}")

    def _stop_simulators(self):
        """Detener los hilos de cada simulador."""
        try:
            if self.rotation_sim:
                self.rotation_sim.stop()
            if self.orbital_sim:
                self.orbital_sim.stop()
            if self.magnetic_sim:
                self.magnetic_sim.stop()
        except Exception as e:
            print(f"Warning: Error stopping simulators: {e}")

    def reset(self, seed=None, options=None):
        """
        Funci贸n para reiniciar el entorno y comenzar un nuevo episodio.
        Args:
            seed (int): Semilla aleatoria para reproducibilidad
            options (dict): Opciones adicionales (no utilizadas)

        Returns:
            tuple: (observaci贸n, informaci贸n)
        """
        super().reset(seed=seed)

        # parar simulaciones para luego reiniciarlas para nuevo episodio
        self._stop_simulators()
        self._create_simulators()

        # condiciones iniciales aleatorias o fijas
        initial_angular_velocity = self.np_random.uniform(-1.0, 1.0, size=3)
        # initial_angular_velocity = self.np_random.uniform(-1.0, 1.0, size=3)
        # initial_angular_velocity = np.array([1.0, 0.0, 0.0])

        initial_quat = self.np_random.normal(size=4)
        # initial_quat = np.array([1.0, 0.0, 0.0, 0.0])

        # Setear condiciones iniciales
        initial_quat /= np.linalg.norm(initial_quat)
        self.rotation_sim.angular_velocity = initial_angular_velocity
        self.rotation_sim.quaternion = initial_quat

        # empezar simulaciones con nuevas condiciones
        self._start_simulators()

        # reiniciar tracking
        self.current_step = 0
        self.episode_reward = 0.0

        # esperar a que se reinicie todo, not the best solution pero funciona
        time.sleep(0.1)

        observation = self._get_observation()
        info = {}

        if self.render_mode == 'human':
            self.render()

        return observation, info

    def step(self, action):
        """
        Ejecuta un solo paso en el entorno dentro de un episodio.

        Args:
            action (np.ndarray): Comando en 3 dimensiones representando el torque

        Returns:
            tuple: (observaci贸n, recompensa, terminado, truncado, informaci贸n) /
                   (observation, reward, terminated, truncated, info)
        """
        # mapear accion discreta a vector de torque
        torque_action = self.action_map[action]
        # print("torque action", torque_action)

        ### TEST: Compare with a simple proportional controller
        # G = 1e-3
        # torque_action = -self.rotation_sim.angular_velocity*G
        ###

        #  GUARDAR ESTADO ANTERIOR ANTES DE APLICAR ACCIN
        previous_angular_vel_norm = np.linalg.norm(self.rotation_sim.angular_velocity)

        # aplicar accion de torque al simulador de rotacion
        self.rotation_sim.set_torque(torque_action)

        # Avanzar la simulacion con una granularidad menor
        dt = self.time_step / self.sim_granularity
        for i in range(self.sim_granularity):
            self.current_time += timedelta(seconds=dt) # Avanzar el tiempo en el step definido
            self.rotation_sim.update_simulation(dt) # Actualizar la simulacion

            # obtener nueva observacion
            observation = self._get_observation()
            # Guardar historicos para graficar
            if self._debug:
                # Agregar el torque tambi茅n al historico
                observation = np.concatenate((observation, torque_action))
                self._observation_hist.append(observation)
                # Agregar el tiempo al historico
                self._time_hist.append(self.current_time.timestamp())

        #  CALCULAR RECOMPENSA CON ESTADO ANTERIOR
        reward = self._calculate_reward(torque_action, previous_angular_vel_norm)
        self.episode_reward += reward

        # revisar si es que termino el episodio
        try:
            angular_vel_norm = np.linalg.norm(self.rotation_sim.angular_velocity)
            terminated = angular_vel_norm < self.success_threshold
        except Exception:
            angular_vel_norm = 1.0
            terminated = False

        # revisar si ocurre un timeout episodico
        # parametro "truncated" (revisar docs en gymnasium)
        self.current_step += 1
        truncated = self.current_step >= self.max_steps

        # retornar info adicional
        info = {
            'angular_velocity_norm': angular_vel_norm,
            'episode_reward': self.episode_reward,
            'success': terminated
        }

        if self.render_mode == 'human':
            self.render()

        return observation, reward, terminated, truncated, info

    def _get_observation(self):
        """
        Obtener la observaci贸n actual del simulador.

        Returns:
            np.ndarray: Vector de observaci贸n: [quat(4), angular_vel(3), mag_field(3)]
        """
        try:
            # obtener estado del simulador de rotaci贸n
            quaternion = self.rotation_sim.quaternion.copy()
            angular_velocity = self.rotation_sim.angular_velocity.copy()

            # obtener info del campo magnetico
            try:
                mag_field_data = self.magnetic_sim.send_request('earth_magnetic_field').result()
                # extraer componentes x, y, z y convertir de nT a T
                mag_field_inertial = np.array([
                    mag_field_data['north'],
                    mag_field_data['east'],
                    mag_field_data['vertical']
                ]) * 1e-9

                # rotar campo magnetico de inercial a cuerpo usando quaternion
                mag_field_body = self._rotate_vector_by_quaternion(mag_field_inertial, quaternion)

            except Exception as e:
                print(f"Warning: Could not get magnetic field: {e}")
                mag_field_body = np.zeros(3)

            observation = np.concatenate([
                quaternion,
                angular_velocity,
                mag_field_body
            ]).astype(np.float32)

            return observation

        except Exception as e:
            print(f"Error getting observation: {e}")
            # retornar observacion default en caso de fallo
            return np.zeros(10, dtype=np.float32)

    def _rotate_vector_by_quaternion(self, vector, quaternion):
        """
        Rotar un vector del marco inercial al marco del cuerpo usando un quaternion.
        Wrapper de lo que ya existe en RotationSimulation.

        Args:
            vector (np.ndarray): Vector 3d en el marco inercial
            quaternion (np.ndarray): Quaternion [qx, qy, qz, qw]

        Returns:
            np.ndarray: Vector rotado en el marco del cuerpo
        """
        try:
            # representar vector como un quaternion puro
            v_quat = np.array([vector[0], vector[1], vector[2], 0.0])

            # conjugado del quaternion (inercial a cuerpo)
            q_conj = np.array([-quaternion[0], -quaternion[1], -quaternion[2], quaternion[3]])

            # Usar el m茅todo est谩tico existente para la multiplicaci贸n de quaterniones
            # rotated_v = q_conj * v_quat * q
            temp = RotationSimulation.quat_mut(q_conj, v_quat)
            rotated_v = RotationSimulation.quat_mut(temp, quaternion)

            # retornar solo la parte del vector
            return rotated_v[:3]

        except Exception as e:
            print(f"Warning: Quaternion rotation failed: {e}")
            return vector

    def _calculate_reward(self, action, previous_angular_vel_norm):
        """
        Funci贸n de recompensa simplificada con reward shaping.
        
        Args:
            action: Acci贸n aplicada (torque)
            previous_angular_vel_norm: Norma de velocidad angular del paso anterior
        """
        try:
            current_angular_vel = self.rotation_sim.angular_velocity
            angular_vel_norm = np.linalg.norm(current_angular_vel)
        except Exception:
            angular_vel_norm = 1.0
        
        control_effort = np.linalg.norm(action)
        
        # Recompensa base: exponencial negativa (m谩s sensible a cambios peque帽os)
        base_reward = -np.exp(angular_vel_norm) + 1.0
        
        # Reward shaping: premiar mejora gradual
        improvement = previous_angular_vel_norm - angular_vel_norm
        shaped_reward = 10.0 * improvement  # Multiplicador alto para cambios peque帽os
        
        # Penalizaci贸n de control reducida
        control_penalty = -0.01 * control_effort
        
        # Bonus por lograr objetivo
        success_bonus = 20.0 if angular_vel_norm < self.success_threshold else 0.0
        
        reward = base_reward + shaped_reward + control_penalty + success_bonus
        
        return reward

    def render(self):
        """
        Renderizar estado actual del entorno.
        """
        if self.render_mode == 'human':
            try:
                quaternion = self.rotation_sim.quaternion
                angular_velocity = self.rotation_sim.angular_velocity
                angular_vel_norm = np.linalg.norm(angular_velocity)

                # print(f"Step: {self.current_step:3d} | "
                #       f"Time: {self.current_time.timestamp():.4f} | "
                #       f"_norm: {angular_vel_norm:.6f} rad/s | "
                #       f": [{angular_velocity[0]:.6f}, {angular_velocity[1]:.6f}, {angular_velocity[2]:.6f}] rad/s | "
                #       f"Episode Reward: {self.episode_reward:.2f} | "
                #       f"Quaternion: [{quaternion[0]:.3f}, {quaternion[1]:.3f}, {quaternion[2]:.3f}, {quaternion[3]:.3f}]")
            except Exception as e:
                print(f"Render error: {e}")

        if self.render_mode == 'plot':
            pass

    def close(self):
        """
        Limpiar entorno y reiniciar todos los simuladores externos.
        """
        if self._plot_hist:
            self.show_hist()
        self._stop_simulators()

    def show_hist(self):
        if len(self._observation_hist) == 0:
            print("No hay historial guardado")
            return

        observation_hist = np.array(self._observation_hist)
        quat_hist = observation_hist[:,0:4]
        vel_hist = observation_hist[:,4:7]
        mag_hist = observation_hist[:,7:10]
        torque_hist = observation_hist[:,10:13]

        figure, axes = plt.subplots(3, 1)
        plt.title("Rotation Simulation")
        axes[0].grid(True)
        axes[1].grid(True)
        plt.ion()
        plt.show(block=False)

        axes[0].clear()
        axes[0].plot(self._time_hist, np.array(vel_hist), "--.", label=["x", "y", "z"])
        axes[0].legend(loc="upper right")
        axes[0].set_ylabel('Velocity (rad/s)')
        axes[0].set_xlabel('Time')
        axes[0].grid(True)

        axes[1].clear()
        axes[1].plot(self._time_hist, np.array(quat_hist), "--.", label=["i", "j", "k", "s"])
        axes[1].legend(loc="upper right")
        axes[1].set_ylabel('Quaternion')
        axes[1].set_xlabel('Time')
        axes[1].grid(True)

        axes[2].clear()
        axes[2].plot(self._time_hist, np.array(torque_hist), "--.", label=["Tx", "Ty", "Tz"])
        axes[2].legend(loc="upper right")
        axes[2].set_ylabel('Torque (%)')
        axes[2].set_xlabel('Time')
        axes[2].grid(True)

        plt.show(block=True)


def test_environment_basic():
    """
    Funci贸n de prueba para mostrar el funcionamiento del entorno.
    """
    print("=" * 60)
    print("Testing CubeSat Detumbling Environment")
    print("=" * 60)

    env = CubeSatDetumblingEnv(render_mode='human')

    try:
        # reiniciar entorno
        obs, _ = env.reset()
        print(f"Initial observation shape: {obs.shape}")
        print(f"Initial observation: {obs}")

        # tomar 10 acciones aleatorias
        for i in range(10):
            action = env.action_space.sample()
            print(f"\nStep {i + 1}: Action = {action}")

            obs, reward, terminated, truncated, _ = env.step(action)
            print(f"Reward: {reward:.4f}")

            if terminated or truncated:
                print(f"Episode ended at step {i + 1}")
                if terminated:
                    print("SUCCESS: Detumbling achieved!")
                else:
                    print("Episode truncated (timeout)")
                break

    except Exception as e:
        print(f"Test error: {e}")
        import traceback
        traceback.print_exc()
    finally:
        env.close()
        print("Environment test completed!")

def evaluate_random_agent(episodes=100, max_steps=400):
    """
    Eval煤a el desempe帽o de un agente aleatorio (l铆nea base) y registra el historial
    de recompensas y 茅xitos por episodio.

    Args:
        episodes (int): N煤mero de episodios para la evaluaci贸n.
        max_steps (int): M谩ximo de pasos por episodio.

    Returns:
        tuple: (m茅tricas_finales, historial_recompensas, historial_exito)
    """
    print("=" * 70)
    print(f"И EVALUACIN DEL AGENTE ALEATORIO (LNEA BASE) - {episodes} EPISODIOS")
    print("=" * 70)

    # Crear una instancia del entorno sin renderizado para la evaluaci贸n
    env = CubeSatDetumblingEnv(render_mode=None, max_steps=max_steps, debug=False, plot_hist=False)
    
    total_rewards = []
    success_count = 0
    steps_to_success = []
    
    # Listas para almacenar el historial de cada episodio
    episode_rewards_hist = []
    episode_success_hist = [] 
    
    start_time = time.time()

    for episode in range(episodes):
        obs, _ = env.reset()
        done = False
        total_reward = 0
        step_count = 0

        while not done:
            # 1. Selecci贸n de Acci贸n: Aleatoria (probabilidad uniforme)
            action = env.action_space.sample()
            
            # 2. Paso en el entorno
            obs, reward, terminated, truncated, info = env.step(action)
            
            total_reward += reward
            step_count += 1
            done = terminated or truncated

        # 3. Registro de m茅tricas y historial
        total_rewards.append(total_reward)
        episode_rewards_hist.append(total_reward)
        
        is_success = terminated
        episode_success_hist.append(is_success)
        
        if terminated:
            success_count += 1
            steps_to_success.append(step_count)
            
        if (episode + 1) % 10 == 0 or episode == episodes - 1:
            print(f"  Episodio {episode + 1}/{episodes} | Recompensa: {total_reward:.2f} | xito: {'S' if terminated else 'NO'}")

    end_time = time.time()
    
    # 4. C谩lculo de M茅tricas Finales
    mean_reward = np.mean(total_rewards)
    std_reward = np.std(total_rewards)
    success_rate = success_count / episodes
    
    if success_count > 0:
        avg_steps_success = np.mean(steps_to_success)
        std_steps_success = np.std(steps_to_success)
    else:
        avg_steps_success = np.nan
        std_steps_success = np.nan

    env.close()

    metrics = {
        'mean_reward': mean_reward,
        'std_reward': std_reward,
        'success_rate': success_rate,
        'avg_steps_on_success': avg_steps_success,
        'std_steps_on_success': std_steps_success,
        'total_time_s': end_time - start_time
    }

    print("\n" + "=" * 70)
    print(" RESUMEN DE MTRICAS (LNEA BASE ALEATORIA)")
    print("-" * 70)
    print(f"Recompensa Promedio (渭): **{metrics['mean_reward']:.2f}**")
    print(f"Desv. Est谩ndar ():       {metrics['std_reward']:.2f}")
    print(f"Tasa de xito:            **{metrics['success_rate'] * 100:.2f}%**")
    print(f"Pasos Prom. al xito:     {metrics['avg_steps_on_success']:.1f} (solo si hay 茅xitos)")
    print("-" * 70)
    print(f"Tiempo total de simulaci贸n: {metrics['total_time_s']:.2f} segundos")
    print("=" * 70)

    # Retornar las m茅tricas finales y los historiales
    return metrics, episode_rewards_hist, episode_success_hist

def plot_random_agent_history(episode_rewards, episode_success, window_size=10):
    """
    Genera gr谩ficos de la ejecuci贸n del agente aleatorio.

    Args:
        episode_rewards (list): Lista de recompensas acumuladas por episodio.
        episode_success (list): Lista de booleanos indicando 茅xito por episodio.
        window_size (int): Tama帽o de la ventana para la media m贸vil.
    """
    episodes = np.arange(1, len(episode_rewards) + 1)
    
    # 1. Suavizar la Recompensa (Media M贸vil)
    rewards_series = np.array(episode_rewards)
    # Media m贸vil simple
    smoothed_rewards = np.convolve(rewards_series, np.ones(window_size)/window_size, mode='valid')
    # Ajustar el eje X para la media m贸vil
    episodes_smoothed = np.arange(window_size, len(episode_rewards) + 1)

    # 2. Calcular la Tasa de xito Acumulada
    success_rate_cumulative = np.cumsum(episode_success) / episodes

    # 3. Graficar
    fig, axes = plt.subplots(nrows=2, ncols=1, figsize=(12, 8), sharex=True)
    fig.suptitle('M茅tricas del Agente Aleatorio (L铆nea Base)', fontsize=16, fontweight='bold')

    # --- Gr谩fico 1: Recompensa por Episodio (Suavizada) ---
    axes[0].plot(episodes_smoothed, smoothed_rewards, label=f'Media M贸vil ({window_size} eps)', color='orange')
    axes[0].set_ylabel('Recompensa Acumulada (Media M贸vil)')
    axes[0].set_title('Progreso de Recompensa por Episodio')
    axes[0].grid(True, linestyle='--', alpha=0.7)
    axes[0].legend()

    # --- Gr谩fico 2: Tasa de xito Acumulada ---
    axes[1].plot(episodes, success_rate_cumulative, label='Tasa de xito Acumulada', color='green')
    axes[1].set_ylabel('Tasa de xito Acumulada')
    axes[1].set_xlabel('Episodio')
    axes[1].set_ylim(0, 1.05)
    axes[1].set_yticks(np.arange(0, 1.1, 0.1))
    
    # L铆nea de la tasa de 茅xito final
    if success_rate_cumulative.size > 0:
        final_rate = success_rate_cumulative[-1]
        axes[1].axhline(y=final_rate, color='r', linestyle=':', label=f'Tasa Final: {final_rate:.2f}')
    
    axes[1].grid(True, linestyle='--', alpha=0.7)
    axes[1].legend()

    plt.tight_layout(rect=[0, 0.03, 1, 0.95])
    plt.show(block=True)

    print("\nGr谩ficos de la L铆nea Base generados.")

if __name__ == "__main__":
    """
    Prueba simple en este mismo script.
    En caso de entrenamiento, se recomienda usar el script train_cubesat_detumbling.py.
    """
    print("=" * 60)
    print("CubeSat Detumbling Environment Test")
    print("=" * 60)

    debug = True  # Activar o desactivar gr谩ficos
    plot_hist = True
    start_time = datetime.fromtimestamp(1758566834)
    time_step = 1
    total_time = 15*60
    granularity = 10

    # crear y probar el entorno
    env = CubeSatDetumblingEnv(render_mode='human', start_time=start_time, time_step=time_step, granularity=granularity,
                               debug=debug, plot_hist=plot_hist)

    print("Environment created successfully!")
    print(f"Action space: {env.action_space}")
    print(f"Observation space: {env.observation_space}")

    """
    try:
        # correr un episodio de prueba...
        obs, _ = env.reset()
        print(f"\nInitial observation shape: {obs.shape}")
        print("Running N random steps...")

        for step in np.arange(0, total_time, time_step):
            action = env.action_space.sample()
            print(f"Acci贸n: {action}")
            obs, reward, terminated, truncated, info = env.step(action)
            print(f"Reward: {reward:.4f}")

            if terminated:
                print(f"\nSUCCESS! Episode completed at step {step + 1}")
                break
            elif truncated:
                print(f"\nEpisode truncated at step {step + 1}")
                break

    except Exception as e:
        print(f"Test error: {e}")
        import traceback

        traceback.print_exc()
    finally:
        env.close()
        print("\nEnvironment test completed!")
    

    """
    # --- Configuraci贸n para la L铆nea Base ---
    EPISODES_TO_EVALUATE = 100 
    MAX_STEPS_PER_EPISODE = 400 # Aseg煤rate de que coincida con la configuraci贸n por defecto
    
    print("=" * 60)
    print("CubeSat Detumbling Environment Test")
    print("=" * 60)
    
    try:
        # 1. Ejecutar el Agente Aleatorio y obtener m茅tricas E HISTORIAL
        # Capturamos los 3 valores devueltos: m茅tricas finales, historial de recompensas e historial de 茅xito.
        random_agent_metrics, rewards_hist, success_hist = evaluate_random_agent(
            episodes=EPISODES_TO_EVALUATE, 
            max_steps=MAX_STEPS_PER_EPISODE
        )
        
        # 2. Generar gr谩ficos del historial
        # Usamos los historiales capturados en el paso 1.
        plot_random_agent_history(rewards_hist, success_hist, window_size=10)
        
        # 3. Imprimir los resultados
        print("\n Evaluaci贸n de L铆nea Base completada.")
        
    except Exception as e:
        print(f"Test error: {e}")
        import traceback
        traceback.print_exc()

    # Cierra la instancia inicial del entorno si no se us贸 en el bloque try/except.
    # Si usaste el bloque de prueba comentado anteriormente, esto cerrar铆a su instancia.
    try:
        env.close()
    except Exception:
        pass
    
    # Nota: Se recomienda mantener esta secci贸n de prueba separada del c贸digo 
    # de entrenamiento real para una estructura m谩s limpia.
    